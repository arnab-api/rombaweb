<!doctype html>
<html lang="en">

<head>
    <title>Locating and Editing Factual Associations in Mamba</title>
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="description"
        content="Approximating relation decoding in transormer LMs as simple linear transformations on the subject token." />
    <meta property="og:title" content="Linearity of Relation Decoding in Transformer Language Models" />
    <meta property="og:url" content="https://lre.baulab.info/" />
    <meta property="og:image" content="https://lre.baulab.info/images/lre-thumb.png" />
    <meta property="og:description"
        content="Approximating relation decoding in transormer LMs as simple linear transformations on the subject token." />
    <meta property="og:type" content="website" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Linearity of Relation Decoding in Transformer Language Models" />
    <meta name="twitter:description"
        content="Updating thousands of memories in GPT by directly calculating parameter changes." />
    <meta name="twitter:image" content="https://lre.baulab.info/images/lre-thumb.png" />
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

    <style>
        .relatedthumb {
            float: left;
            width: 200px;
            margin: 3px 10px 7px 0;
        }

        .relatedblock {
            clear: both;
            display: inline-block;
        }

        .bold-sc {
            font-variant: small-caps;
            font-weight: bold;
        }

        .cite,
        .citegroup {
            margin-bottom: 8px;
        }

        :target {
            background-color: yellow;
        }
    </style>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
    </script>

</head>

<body class="nd-docs">
    <div class="nd-pageheader">
        <div class="container">
            <h1 class="lead">
                <nobr class="widenobr">Linearity of Relation Decoding in </nobr>
                <nobr class="widenobr">Transformer LMs</nobr>
            </h1>
            <address>
                <nobr><a href="https://evandez.com/" target="_blank">Evan Hernandez</a><sup>1*</sup>,</nobr>
                <nobr><a href="https://arnab-api.github.io/" target="_blank">Arnab Sen Sharma</a><sup>2*</sup>,</nobr>
                <nobr><a href="https://github.com/TalHaklay" target="_blank">Tal Haklay</a><sup>3</sup>,</nobr>
                <nobr><a href="https://mengk.me/" target="_blank">Kevin Meng</a><sup>1</sup>,</nobr>
                <nobr><a href="https://www.bewitched.com/" target="_blank">Martin Wattenberg</a><sup>4</sup>,</nobr>
                <nobr><a href="https://www.mit.edu/~jda/" target="_blank">Jacob Andreas</a><sup>1</sup>,</nobr>
                <nobr><a href="https://www.cs.technion.ac.il/~belinkov/" target="_blank">Yonatan
                        Belinkov</a><sup>3</sup>,
                </nobr>
                <nobr><a href="https://baulab.info/" target="_blank">David Bau</a><sup>2</sup></nobr>
                <br>
                <nobr><sup>1</sup><a href="https://www.csail.mit.edu/" target="_blank">MIT CSAIL</a>,</nobr>
                <nobr><sup>2</sup><a href="https://khoury.northeastern.edu/" target="_blank">Northeastern
                        University</a>,</nobr>
                <nobr><sup>3</sup><a href="https://www.cs.technion.ac.il/" target="_blank">Technion - IIT</a>;</nobr>
                <nobr><sup>*</sup>Equal contribution</nobr>
            </address>
        </div>
    </div><!-- end nd-pageheader -->

    <div class="container">
        <div class="row justify-content-center" style="margin-bottom: 20px">
            <!-- <p class="text-center">
<a href="https://lre.baulab.us/"
   >New!  Try interacting with a lre-edited GPT to see the effect of inserting hundreds of memories.</a>
</p> -->
        </div>
        <div class="row justify-content-center text-center">

            <p>
                <a href="https://arxiv.org/pdf/2308.09124.pdf" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/paper-thumb.jpg" style="border:1px solid; margin: 0 38px;"
                        alt="ArXiv Preprint thumbnail" data-nothumb="">
                    <br>ArXiv<br>Preprint</a>
                <a href="https://github.com/evandez/relations" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;"
                        alt="Github code thumbnail" data-nothumb="">
                    <br>Source Code<br>
                </a>
                <a href="data" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/data-thumb.png" style="border:1px solid; margin: 0 38px;"
                        alt="Dataset thumbnail" data-nothumb="">
                    <br>Dataset<br>
                </a>
            </p>

            <div class="card" style="max-width: 1020px;">
                <div class="card-block">
                    <h3>How do Transformer LMs Decode Relations?</h3>
                    <p style="text-align: justify;">Much of the knowledge contained in neural language models may be
                        expressed in terms of relations.
                        For example, the fact that Miles Davis is a trumpet player can be written as a <b>relation</b>
                        (<i>plays the
                            instrument</i>&nbsp;)
                        connecting a <b>subject</b> (<i>Miles Davis</i>&nbsp;) to an <b>object</b>
                        (<i>trumpet</i>&nbsp;).
                    </p>
                    <p style="text-align: justify;">
                        One might expect how a language model decodes a relation to be a sequence of complex, non-linear
                        computation spanning multiple layers. However, in this paper we show that for a subset of
                        relations this (highly non-linear) decoding procedure can be well-approximated by a single
                        <i>linear
                            transformation</i> (<b style="font-family:'Times New Roman'; font-size:19px">LRE</b>) on the
                        subject representation <b style="font-family:'Times New Roman'; font-size:19px">s</b> after some
                        intermediate layer.

                    </p>
                </div><!--card-block-->
            </div><!--card-->

        </div><!--row-->

        <div class="row">
            <div class="col">
                <figure class="center_image" style="margin-top: 30px">
                    <img src="images/Paper/schematic-wide.png" style="width:100%">
                    <figcaption>
                        In an LM relations such as <em>plays the instrument</em> can be well-approximated by a linear
                        function
                        <b style="font-family:'Times New Roman'; font-size:19px"><i>R</i></b> that maps subject
                        representation
                        <b style="font-family:'Times New Roman'; font-size:19px">s</b> to object representation
                        <b style="font-family:'Times New Roman'; font-size:19px">o</b>, which is then directy decoded.
                    </figcaption>
                </figure>

                <h2>How to get the LRE approximating a relation decoding?</h2>

                <p style="text-align: justify;">A linear approximation in form of
                    <span style="font-family:'Times New Roman'; font-size:20px">
                        LRE(<b>s</b>) = <i>W</i><b>s</b> + <i>b</i>
                    </span>
                    can be obtained by taking a first order Taylor series approximation to the LM computation, where
                    <span style="font-family:'Times New Roman'; font-size:20px"><i>W</i></span>
                    is the local derivative (Jacobian) of the LM computation at some subject representation
                    <span style="font-family:'Times New Roman'; font-size:19px"><b>s<sub>0</sub></b></span>.
                    For a range of relations we find that averaging the estimation of
                    <span style="font-family:'Times New Roman'; font-size:19px">LRE</span>
                    parameters on just 5 samples is enough to get a faithful approximation of LM decoding.

                <figure>
                    <img src="images/Paper/LRE_eqn.png" class="center_image" style="width: 30%;">
                </figure>

                <p style="text-align: justify;">
                    Here
                    <span style="font-family:'Times New Roman'; font-size:20px"><i>F</i></span>
                    represents how LM obtains the object representation
                    <span style="font-family:'Times New Roman'; font-size:19px"><b>o</b></span>
                    from the subject representation
                    <span style="font-family:'Times New Roman'; font-size:19px"><b>s</b></span>
                    introduced within a textual context
                    <span style="font-family:'Times New Roman'; font-size:20px"><i>c</i></span>.
                    Kindly refer to our paper for further details.
                </p>

                <h2>How well is the LRE approximation?</h2>

                <p style="text-align: justify;">
                    We evaluate the LRE approximations on a set of 47 relations spanning 4 categories:
                    <em>factual associations</em>, <em>commonsense knowledge</em>, <em>implicit biases</em>, and
                    <em>linguistic
                        knowledge</em>.
                    We find that for almost half of the relations LRE faithfully recovers subject-object mappings for a
                    majority
                    of the subjects in the test set.
                </p>
                <p style="text-align: justify;">
                    We also identify a set of relations where we couldn't find a good LRE approximations. For most of
                    these
                    relations
                    the range was names of people and companies. We think the range for this relations are so large that
                    LM cannot
                    encode them in a single state, and relies on a more complex non-linear decoding procedure.
                </p>

                <figure>
                    <img src="images/Paper/faithfulness_relationwise.jpg" class="center_image" style="width: 80%;">
                </figure>

                <h2>Attribute Lens</h2>
                Attribute Lens, which is motivated by the idea that a hidden state
                <span style="font-family:'Times New Roman'; font-size:20px"><b>h</b></span>
                may contain pieces of information beyond the prediction of the immediate next token. And, an
                <span style="font-family:'Times New Roman'; font-size:19px">LRE</span>
                can be used to extract a certain attribute from
                <span style="font-family:'Times New Roman'; font-size:20px"><b>h</b></span>
                without relevant textual context.

                <figure>
                    <img src="images/Paper/attribute_lens.png" class="center_image" style="width: 80%;">
                    <figcaption>
                        <span style="font-family:'Times New Roman'; font-size:16px">LRE</span>
                        approximating the relation <em>country capital</em> applied on hidden state
                        <b style="font-family:'Times New Roman'; font-size:16px">h</b>
                        after different layers in different token positions.
                    </figcaption>
                </figure>


                <h2>How to cite</h2>

                <p>This work is not yet peer-reviewed. The preprint can be cited as follows.
                </p>

                <div class="card">
                    <h3 class="card-header">bibliography</h3>
                    <div class="card-block">
                        <p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
                            Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Yonatan
                            Belinkov, and David Bau. "<em>Linearity of Relation Decoding in Transformer Language
                                Models.</em>" Proceedings of the 2024 International Conference on Learning
                            Representations. <nobr>(ICLR 2024 spotlight)</nobr>
                        </p>
                    </div>
                    <h3 class="card-header">bibtex</h3>
                    <div class="card-block">
                        <pre class="card-text clickselect">
@inproceedings{hernandez2024linearity,
    title={Linearity of Relation Decoding in Transformer Language Models}, 
    author={Evan Hernandez and Arnab Sen Sharma and Tal Haklay and Kevin Meng and Martin Wattenberg and Jacob Andreas and Yonatan Belinkov and David Bau},
    booktitle={Proceedings of the 2024 International Conference on Learning Representations},
    year={2024},
}
</pre>
                    </div>
                </div>
                </p>

            </div>
        </div><!--row -->
    </div> <!-- container -->

    <footer class="nd-pagefooter">
        <div class="row">
            <div class="col-6 col-md text-center">
                <a href="https://baulab.info/">About the Bau Lab</a>
            </div>
        </div>
    </footer>

</body>
<script>
    $(document).on('click', '.clickselect', function (ev) {
        var range = document.createRange();
        range.selectNodeContents(this);
        var sel = window.getSelection();
        sel.removeAllRanges();
        sel.addRange(range);
    });
</script>

</html>